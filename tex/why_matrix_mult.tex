\documentclass{article}
% \usepackage{fonttable,textcomp}
% \usepackage[T1]{fontenc}
% \usepackage{NewYorker}
\usepackage{amsmath,amscd}

\title{Origins of Matrix/Vector and Matrix/Matrix Multiplication}
\author{R. Scott McIntire}
\date{Dec 6, 2024}
\setlength{\textwidth}{6.5in}
\setlength{\textheight}{7.5in}
\setlength{\hoffset}{-0.75in}
\setlength{\voffset}{-0.75in}

\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.5\baselineskip}


\begin{document}
\maketitle


\section{Algebraic properties of Matrices}
Why is matrix/vector and matrix/matrix multiplication defined the way it is?
One motivation is to try to extend the (albeit trivial) solution of linear
equations in 1-dimension to n dimensions. To do this, we go through the solution
of the 1-dimensional case in careful detail.

The scalar problem is:
\begin{eqnarray}
  a \, x & = & b \label{scalar-problem}
\end{eqnarray}

Here is a very explicit solution keeping an eye towards generalization.
\begin{eqnarray}
  a \, x & = & b \\
  a^{-1} ( a \, x ) & = & a^{-1} b \quad \quad \text{(Multiply by Inverse)} \label{inv} \\
  (a^{-1} a) x & = & a^{-1} b \quad \quad \text{(Use associativity of multiplication)} \label{assoc} \\
  1 \, x & = & a^{-1} b \quad \quad \text{(What Inverse multiplication does)} \label{inv-mult} \\
  x & = & a^{-1} b \quad \quad \text{(What identity multiplication does)} \label{identity}
\end{eqnarray}

The multi-dimensional problem has a lot more variables and coefficients.
To start, we need to be more systematic about the naming of these coefficients.
With this in mind, the multi-dimensional linear problem can be written:
\begin{eqnarray*}
  a_{11} x_1 + a_{12} x_2 + \cdots + a_{1n} x_n & = & b_1 \\
  a_{21} x_1 + a_{22} x_2 + \cdots + a_{2n} x_n & = & b_2 \\
  \cdots & = & \cdots \\
  a_{n1} x_1 + a_{n2} x_2 + \cdots + a_{nn} x_n & = & b_n 
\end{eqnarray*}
Here, $a_{ij}$ is the coefficient in the $i^{\rm th}$ row and $j^{\rm th}$ column.


To make this look like the 1-dimensional case, we need to think of the $b$'s as
a single unit. Our single unit will be the {\em vector\/} of the $b$'s.

The multi-dimensional case can be rewritten in vector terms as:
\begin{eqnarray}
  \left[
  \begin{array}{c}
  a_{11} x_1 + a_{12} x_2 + \cdots + a_{1n} x_n \\
  a_{21} x_1 + a_{22} x_2 + \cdots + a_{2n} x_n \\
  \cdots  \\
    a_{n1} x_1 + a_{n2} x_2 + \cdots + a_{nn} x_n
  \end{array}
  \right] & = & \left[
                 \begin{array}{c}
                   b_1 \\
                   b_2 \\
                   \vdots \\
                   b_n
                 \end{array}
   \right] \label{vector-form}
\end{eqnarray}
We can replace the tedious list of sums on the left hand side of the above
with standard mathematical summing notation:
\begin{eqnarray}
  \left[
  \begin{array}{c}
	\sum_{j=1}^n a_{1, j} x_j \\ 
	\sum_{j=1}^n a_{2, j} x_j \\ 
	\cdots \\
	\sum_{j=1}^n a_{n, j} x_j \\ 
  \end{array}
  \right] & = & \left[
                 \begin{array}{c}
                   b_1 \\
                   b_2 \\
                   \vdots \\
                   b_n
                 \end{array}
   \right] \label{vector-form1}
\end{eqnarray}
Two things need to be done: treat the $x$'s as a unit -- as we did with the
$b$'s -- and separate the $a$ coefficients from the $x$'s. This must be done formally
and yet have the same meaning as the original problem formulation.
This is done in the most natural of ways:
\begin{eqnarray}
  \left[
  \begin{array}{cccc}
    a_{11} & a_{12} & \cdots & a_{1n} \\
    a_{21} & a_{22} & \cdots & a_{2n} \\
    \vdots \\
    a_{n1} & a_{n2} & \cdots & a_{nn} \\
    \end{array}
  \right]
  \left[
  \begin{array}{c}
    x_1 \\
    x_2 \\
    \vdots \\
    x_n
  \end{array}
  \right] & = &
                \left[
                \begin{array}{c}
                  b_1 \\
                  b_2 \\
                  \vdots \\
                  b_n
                \end{array}
  \right] \label{matrix-form2}
\end{eqnarray}
Taking the rectangular collection (matrix) of $a$'s to be represented by 
$A$; the (vector) collection
of $x$'s to be ${\bf x}$; and the collection of the $b$'s to be ${\bf b}$;
we may write $(\ref{matrix-form2})$ as:%
\footnote{In general, we use uppercase italic letters for matrices; lowercase bold italic letters
for vectors; and lowercase italic letters for scalars.}
\begin{eqnarray}
  A \, {\bf x} & = & {\bf b} \label{concise-matrix-form}
\end{eqnarray}
This now looks like the scalar problem, $(\ref{scalar-problem})$.
Just as one writes $a \, x = b$, interpreting the proximity of $a$ and $x$ to mean
multiplication, we want the proximity of $A$ and ${\bf x}$ to mean matrix/vector
multiplication -- something we need to work out.

We proceed to try solving $(\ref{concise-matrix-form})$ using the solution 
procedure used above for the scalar case. In the process, we need to:
\begin{itemize}
    \item{Vectorize the input variable $x$ and the outputs $b$. -- {\bf Done}.}
    \item{Define an, ``$a$'', matrix. -- {\bf Done}.}
    \item{Define matrix-vector multiplication.}
    \item{Define matrix-matrix multiplication.}
    \item{Define the identity matrix.}
    \item{Define the inverse matrix.}
\end{itemize}
How do we make sense of this matrix/vector syntax on the left hand side of 
$(\ref{concise-matrix-form})$? It should have
the proper meaning; that is, $(\ref{matrix-form2})$
should have the same meaning as $(\ref{vector-form1})$.

Examining $(\ref{matrix-form2})$ and the left hand side of
$(\ref{vector-form1})$, gives us our definition of matrix/vector multiplication:
\begin{eqnarray}
  \left[A \, {\bf x}\right]_i & \equiv & \sum_{j=1}^n A_{ij} x_j
                                         \label{matrix-vector-mult}
\end{eqnarray}
That is, $A$ acts on a vector, ${\bf x}$, to create a new vector, $A \, {\bf x}$,
whose $i^{\rm th}$ entry is the $i^{\rm th}$ entry of
the left hand side of $(\ref{vector-form})$.

Another way to write this is:
\begin{eqnarray}
  A {\bf x} & = & \sum_{j=1}^n x_j {\bf A}^j
\end{eqnarray}
Here, ${\bf A}^j$ is the $j^{\rm th}$ column vector of $A$. Applying $A$ to the special
vectors, ${\bf e}_j$
-- which are $0$ everywhere except at $j$ where they are $1$ --
we see that $A \, {\bf e}_j = {\bf A}^j$. 
Consequently, this matrix/vector
multiplication determines $A$ uniquely
-- if there is another matrix, its columns would have to match $A$.

We note the following for future reference.
\begin{eqnarray}
	[{\bf A}^j]_i = A_{i, j} \label{vec-matrix-index}
\end{eqnarray}
That is, the $i^{\rm th}$ entry of the column vector, ${\bf A}^j$, is the
$i^{\rm th}$ row,  $j^{\rm th}$ column of the matrix $A$. 

Before moving on with our outline, we note one very important property of 
matrix/vector multiplication: It is {\em linear\/}.
By that we mean the following:%
\footnote{Here, we assume the reader has a passing knowledge of how vectors are added and multiplied by scalars.}
\begin{eqnarray}
	A( a {\bf x} + b{\bf y} ) & = & a\, A{\bf x} + b\, A{\bf y}
\end{eqnarray}
One can show this from our definition of matrix/vector multiplication which we
leave to the reader. What this means in practice is that for a given vector, ${\bf z} = a{\bf x} + b{\bf y}$,
we can compute $A {\bf z}$ by computing $A$ on the ``components'' of ${\bf z}$ and then multiplying by scalars and adding
the resulting vectors.
This can be an easier way to compute the action of a matrix on a vector in some cases.
The linearity result above can be extended to arbitrary sums as:
\begin{eqnarray}
	A\left( \sum_{j=1}^n c_j {\bf x}_j \right) & = & \sum_{j=1}^n c_j A {\bf x}_j \label{linearity}
\end{eqnarray}

Continuing with our solution outline, matrix/matrix multiplication is needed
to go from equation $(\ref{inv})$ to $(\ref{assoc})$. How must this be defined?
Well, we need to make sense of:
\begin{eqnarray}
  (A^{-1} A) {\bf x} & = & A^{-1} (A \, {\bf x}) 
\end{eqnarray}
This involves the inverse of the matrix $A$, which we also need to define. 
What we are imposing on matrix multiplication is that it be associative in the
specific case of a matrix and its inverse. But this is not what happens in the scalar case;
associativity works not just for a special case of multiplication, but for all numbers.
We will forgo what the inverse of a matrix might mean and focus now on imposing the 
condition that matrix/matrix multiplication be associative -- just as multiplication is for {\em all\/} numbers. 
Specifically, the condition we impose is:
\begin{eqnarray}
	(B\, A) {\bf x} & = & B (A \, {\bf x}) \quad \forall {\bf x} \in R^n \label{ASSOC}
\end{eqnarray}
Notice that we are imposing what matrix/matrix multiplication is by saying how the 
new matrix formed, $B \, A$, acts on an {\em arbitrary\/} vector, ${\bf x}$. And we 
specify that action by the right hand side of $(\ref{ASSOC})$, which involves only matrix/vector
multiplication -- something we already know.

We want to emphasize that this is a strong condition to impose; meaning, that the 
requirement that this is true for {\em all\/} ${\bf x}$ gives us a lot to work with.
For instance, if one had two $n\times n$ matrices, $C$ and $D$, what could you conclude if someone
told you that $C {\bf x} = D {\bf x}$ for {\em some\/} $n$ vector, ${\bf x}$?
Well, the answer is: not much.
However, if I told you that $C {\bf x} = D {\bf x}$ {\em for all\/} ${\bf x}$, what could you say?
I claim that tells us that $C \equiv D$; meaning that every entry in $C$ is the same as the corresponding
entry in $D$. We can see this by applying $C$ and $D$ to each of the ${\bf e}_j$, ($j \in [1, n]$) vectors.
For each $j$, $C {\bf e}_j$ and $D {\bf e}_j$ pick off the $j^{\rm th}$ column of 
$C$ and $D$ respectively. So we see that each column of $C$ and $D$ must match; consequently,
$C \equiv D$.

We apply this idea to $(\ref{ASSOC})$ to find a formula for matrix/matrix multiplication.
Since $(\ref{ASSOC})$ must be true {\em for all\/} ${\bf x}$; in particular, it must be
true for the vectors $\{{\bf e}_j\}_{j=1}^n$.
This gives us the following equations:
\begin{eqnarray}
	(B \, A) {\bf e}_j & = & B (A \, {\bf e}_j) \quad\quad \text{$j \in [1,n]$} 
\end{eqnarray}
Or,
\begin{eqnarray}
	({\bf B} \, {\bf A})^j & = & B  {\bf A}^j \quad\quad \text{$j \in [1, n]$}
\end{eqnarray}
For any given, $i$, the $i^{\rm th}$ entry of the left and right hand side is
(after using the definition of matrix/vector 
multiplication, $(\ref{matrix-vector-mult})$, on the right hand side)
\begin{eqnarray}
	\left[({\bf B} \, {\bf A})^j\right]_i & = & \sum_{k=1}^n B_{i,k}  \left[{\bf A}^j\right]_k  \quad \forall i,j \quad i \in [1, n] \wedge j \in [1.n]
\end{eqnarray}
Using $(\ref{vec-matrix-index})$ on the left and right hand sides of the previous equation we have
\begin{eqnarray}
	(B \, A)_{i, j} & = & \sum_{k=1}^n B_{i, k}  A_{k, j}  \quad \forall i,j \quad i \in [1, n] \wedge j \in [1.n] \label{matrix-matrix-mult}
\end{eqnarray}
And we are done, we have shown what the new matrix, $B \, A$ is by showing what every entry, $(B\, A)_{i, j}$, of
the new matrix is.
So using the set of vectors, $\{{\bf e}_j\}_{j=1}^n$, we have completely determined what 
the matrix multiplication of $B \, A$ is in order that matrix/matrix multiplication be 
associative when using these vectors. Does this mean that associativity works for 
{\em arbitrary\/} vectors? The answer is yes and this is because of the linearity of
matrix/vector multiplication. Any given ${\bf x}$ can be written as a {\em linear combination\/}
of the vectors $\{{\bf e}_j\}_{j=1}^n$: ${\bf x} = a_1 {\bf e}_1 + a_2 {\bf e_2} + \cdots + a_n {\bf e}_n$.
Matrix multiply the left and right hand sides by the matrix, $B\,A$. This gives
\begin{eqnarray}
	(B\, A){\bf x} & = & (B \, A)\left(\sum_{j=1}^n a_j {\bf e}_j\right) \quad\quad \text{Replace ${\bf x}$ with its components.}\nonumber \\
				   & = &\sum_{j=1}^n a_j (B \, A){\bf e}_j \quad\quad \text{{\em Linearity\/} of matrix, $(B \, A)$.}\nonumber \\
				   & = &\sum_{j=1}^n a_j B (A{\bf e}_j) \quad\quad \text{{\em Associativity\/} of matrix multiplication on ${\bf e}_j$.}\nonumber \\
				   & = &B\left(\sum_{j=1}^n a_j A{\bf e}_j\right) \quad\quad \text{{\em Linearity\/} of matrix $B$.}\nonumber \\
				   & = &B\left( A\left(\sum_{j=1}^n a_j {\bf e}_j\right)\right) \quad\quad \text{{\em Linearity\/} of matrix $A$.}\nonumber \\
				   & = &B( A {\bf x}) \quad\quad \text{Replace components with ${\bf x}$.}
\end{eqnarray}
In the above calculations, we applied our new matrix, $(B \, A)$ to an arbitrary ${\bf x}$, and 
used the {\em linearity\/} of matrix/vector multiplication and the {\em associativity\/}
of matrix/matrix multiplication on the components, to show that matrix/matrix
multiplication, as we have defined it, is associative -- irrespective of the input vector.

The next order of business is to identify an $n \times n$ matrix which serves as an
identity (in terms of matrix/vector multiplication).
Let us suppose that we have such a matrix and let's call it $I$.
Then we must have $I \, {\bf x} = {\bf x} \quad \forall {\bf x} \in R^n$. 
Then, in particular,  
$I \, {\bf e}_{j} = {\bf e}_j \quad \forall j \in [1, n]$. 
However, we know that $I \, {\bf e}_j = I^j$. Therefore, $I$
must have the property that ${\bf I^j} = {\bf e}_j$. Consequently, $I$ must have the form:
\begin{eqnarray}
  I & = & \left[
      \begin{array}{cccccc}
        1 & 0 & 0 & 0 & \cdots & 0 \\
        0 & 1 & 0 & 0 & \cdots & 0 \\
        0 & 0 & 1 & 0 & \cdots & 0 \\
        0 & 0 & 0 & 0 & \ddots & 0 \\
        0 & 0 & 0 & 0 & \cdots & 1 \\
      \end{array}
  \right] \label{identity-matrix}
\end{eqnarray}
We have shown that the only candidate for the identity matrix with respect to 
matrix/vector multiplication is the matrix $I$. That is, if there is an 
identity matrix, it must be the matrix of equation $(\ref{identity-matrix})$. Does it satisfy
the property of being an identity matrix (again, in the matrix/vector multiplication world)?
That is, do we have $I \, {\bf x} = {\bf x}$ for all ${\bf x} \in R^n$?
As before, we can express ${\bf x}$ as: ${\bf x} = \sum_{j=1}^n c_j {\bf e}_j$.
By the {\em linearity\/} of matrix/vector multiplication, we have
\begin{eqnarray}
	I {\bf x} & = & I\left( \sum_{j=1}^n c_j {\bf e}_j \right)  \quad\quad \text{Replace ${\bf x}$ with components.} \nonumber \\
							 & = & \sum_{j=1}^n c_j  I{\bf e}_j \quad\quad \text{{\em Linearity\/} of matrix/vector multiplication.} \nonumber \\
							 & = & \sum_{j=1}^n c_j {\bf e}_j \quad\quad \text{{\em Identity\/} of $I$ on components.}\nonumber \\
							 & = & {\bf x} \quad\quad \text{Replace components with ${\bf x}$.}
\end{eqnarray}
One can show that this identity matrix, $I$, is also the identity operator for
matrix/matrix multiplication.

The only thing left is to know when a matrix inverse exists and how to compute it.
We do not attempt to do this in this paper. In the next section we continue with
a qualitative comparison of the solution to the scalar problem, $a\, x = b$, 
and its vectorized cousin.


\section{Qualitative Features of Solutions}
We can view the multiplication of two numbers, $a$ and $x$, as just that.
Or, we can think of $a$ being fixed and letting $x$ "run-through" all 
numbers. Here we see two cases: if $a \neq 0$, then letting $x$ run
through all of the numbers in $R$ will produce all of the numbers in $R$.
We could think of $a$ as an "operator" and call the set of all possible 
outputs, the range of $a$ and denote it: ${\cal R}(a)$.
There is another case, $a$ could be zero. In this case it's range is the
set $\{0\}$. In the first case, with non-zero $a$ it is clear that we can
find an $x$ to ``hit'' a given value $b$. That is, we can solve $a\, x = b$.

One can define the same concept for a matrix, $A$. Using this language
of ranges, here is what we can say about the scalar problem: $a \, x = b$.
\begin{description}
    \item[Unique Solution:]{If $a^{-1}$ exists ($a \neq 0$), $b$ is any number (that is: $(b \in {\cal R}(a))$}
		then there is a {\bf unique\/} solution.
\item[No Solution:]{If $a^{-1}$ does not exist (i.e., $a = 0$) {\em AND\/}
	$b$ is {\bf not\/} in the range of $a$ (that is: $b \ne 0$), then there is {\bf no\/} solution.}
\item[Infinite Solutions:]{If $a^{-1}$ does not exist (i.e., $a = 0$) {\em BUT\/}
	$b$ is in the range of $a$ (that is: $b = 0$), 
	then there are an {\bf infinite\/} number of solutions.}
\end{description}

Here is the analog of this solution categorization for the multi-dimensional case: $A \, {\bf x} = {\bf b}$.
\begin{description}
	\item[Unique Solution:]{If $A^{-1}$ exists, ${\bf b\/}$ is in the range of $A$ (${\bf b} \in R(A)$)
		then there is a {\bf unique\/} solution.}
\item[No Solution:]{If $A^{-1}$ does not exist {\em AND\/}
	${\bf b\/}$ is not in the range of $A$ (that is ${\bf b} \notin {\cal R}(A)$),
then there is {\bf no\/} solution.}  
\item[Infinite Solutions:]{If $A^{-1}$ does not exist {\em BUT\/}
		${\bf b\/}$ is in the range of $A$ (that is: ${\bf b} \in {\cal R}(A)$),
then there are an {\bf infinite\/} number of solutions.} 
\end{description}

\end{document}

