\documentclass{article}
% \usepackage{fonttable,textcomp}
% \usepackage[T1]{fontenc}
% \usepackage{NewYorker}
\usepackage{amsmath,amscd}

\title{Origins of Matrix/Vector and Matrix/Matrix Multiplication}
\author{R. Scott McIntire}
\date{Dec 4, 2024}
\setlength{\textwidth}{6.5in}
\setlength{\textheight}{7.5in}
\setlength{\hoffset}{-0.75in}
\setlength{\voffset}{-0.75in}

\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.5\baselineskip}


\begin{document}
\maketitle


\section{Algebraic properties of Matrices}
Why is matrix/vector and matrix/matrix multiplication defined the way it is?
One motivation is to try to extend the (albeit trivial) solution of linear
equations in 1-dimension to n dimensions. To do this, we go through the solution
of the 1-dimensional case in careful detail.

The scalar problem is:
\begin{eqnarray}
  a \, x & = & b \label{scalar-problem}
\end{eqnarray}

Here is a very explicit solution keeping an eye towards generalization.
\begin{eqnarray}
  a \, x & = & b \\
  a^{-1} ( a \, x ) & = & a^{-1} b \quad \quad \text{(Multiply by Inverse)} \label{inv} \\
  (a^{-1} a) x & = & a^{-1} b \quad \quad \text{(Use associativity of multiplication)} \label{assoc} \\
  1 \, x & = & a^{-1} b \quad \quad \text{(What Inverse multiplication does)} \label{inv-mult} \\
  x & = & a^{-1} b \quad \quad \text{(What identity multiplication does)} \label{identity}
\end{eqnarray}

The multi-dimensional problem has a lot more variables and coefficients.
To start, we need to be more systematic about the naming of these coefficients.
With this in mind, the multi-dimensional linear problem can be written:
\begin{eqnarray*}
  a_{11} x_1 + a_{12} x_2 + \cdots + a_{1n} x_n & = & b_1 \\
  a_{21} x_1 + a_{22} x_2 + \cdots + a_{2n} x_n & = & b_2 \\
  \cdots & = & \cdots \\
  a_{n1} x_1 + a_{n2} x_2 + \cdots + a_{nn} x_n & = & b_n 
\end{eqnarray*}
Here, $a_{ij}$ is the coefficient in the $i^{\rm th}$ row and $j^{\rm th}$ column.


To make this look like the 1-dimensional case, we need to think of the $b$'s as
a single unit. Our single unit will be the {\em vector\/} of the $b$'s.

The multi-dimensional case can be rewritten in vector terms as:
\begin{eqnarray}
  \left[
  \begin{array}{c}
  a_{11} x_1 + a_{12} x_2 + \cdots + a_{1n} x_n \\
  a_{21} x_1 + a_{22} x_2 + \cdots + a_{2n} x_n \\
  \cdots  \\
    a_{n1} x_1 + a_{n2} x_2 + \cdots + a_{nn} x_n
  \end{array}
  \right] & = & \left[
                 \begin{array}{c}
                   b_1 \\
                   b_2 \\
                   \vdots \\
                   b_n
                 \end{array}
   \right] \label{vector-form}
\end{eqnarray}
Two things need to be done: treat the $x$'s as a unit -- as we did with the
$b$'s -- and separate the $a$ coefficients from the $x$'s. This must be done formally
and yet have the same meaning as the original problem formulation.
This is done in the most natural of ways:
\begin{eqnarray}
  \left[
  \begin{array}{cccc}
    a_{11} & a_{12} & \cdots & a_{1n} \\
    a_{21} & a_{22} & \cdots & a_{2n} \\
    \vdots \\
    a_{n1} & a_{n2} & \cdots & a_{nn} \\
    \end{array}
  \right]
  \left[
  \begin{array}{c}
    x_1 \\
    x_2 \\
    \vdots \\
    x_n
  \end{array}
  \right] & = &
                \left[
                \begin{array}{c}
                  b_1 \\
                  b_2 \\
                  \vdots \\
                  b_n
                \end{array}
  \right] \label{matrix-form}
\end{eqnarray}
Taking the rectangular collection (matrix) of $a$'s to be $A$; the (vector) collection
of $x$'s to be ${\bf x}$; and the collection of the $b$'s to be ${\bf b}$;
we may write $(\ref{matrix-form})$ as:
\begin{eqnarray}
  A \, {\bf x} & = & {\bf b} \label{concise-matrix-form}
\end{eqnarray}
This now looks like the scalar problem, $(\ref{scalar-problem})$.

We proceed to try solving using the solution procedure used above for the scalar case.
In the process, we will need to:
\begin{itemize}
    \item{Vectorize the input variable $x$ and the outputs $b$. -- {\bf Done}.}
    \item{Define an, $a$, matrix. -- {\bf Done}.}
    \item{Define matrix-vector multiplication.}
    \item{Define matrix-matrix multiplication.}
    \item{Define the identity matrix.}
    \item{Define inverse matrix.}
\end{itemize}

How do we make sense of this matrix/vector syntax? It should have
the proper meaning; that is, $(\ref{matrix-form})$
should have the same meaning as $(\ref{vector-form})$.

Examining $(\ref{matrix-form})$ and the left hand side of
$(\ref{vector-form})$, gives us our definition of matrix/vector multiplication:
\begin{eqnarray}
  \left[A \, {\bf x}\right]_i & \equiv & \sum_{j=1}^n A_{ij} x_j
                                         \label{matrix-vector-mult}
\end{eqnarray}
That is, $A$ acts on a vector, ${\bf x}$, to create a new vector, $A \, {\bf x}$,
whose $i^{\rm th}$ entry is the $i^{\rm th}$ entry of
the left hand side of $(\ref{vector-form})$.

Another way to write this is:
\begin{eqnarray}
  A {\bf x} & = & \sum_{j=1}^n x_j {\bf A}^j
\end{eqnarray}
Here, ${\bf A}^j$ is the $j^{\rm th}$ column vector of $A$. Applying $A$ to the special
vectors, ${\bf e}_j$
-- which are $0$ everywhere except at $j$ where they are $1$ --
we see that $A \, {\bf e}_j = {\bf A}^j$. 
Consequently, this matrix/vector
multiplication determines $A$ uniquely
-- if there is another matrix, its columns would have to match $A$.

We note the following for future reference.
\begin{eqnarray}
	[{\bf A}^j]_i = A_{i, j} \label{vec-matrix-index}
\end{eqnarray}
That is, the $i^{\rm th}$ entry of the column vector, ${\bf A}^j$, is the
$i^{\rm th}$ row,  $j^{\rm th}$ column of the matrix $A$. 

To complete the solution outline, matrix/matrix multiplication is needed
to go from equation $(\ref{inv})$ to $(\ref{assoc})$. How must this be defined?
Well, we need to make sense of:
\begin{eqnarray}
  (A^{-1} A) {\bf x} & = & A^{-1} (A \, {\bf x}) 
\end{eqnarray}
This involves the inverse of the matrix $A$, which we also need to define. 
What we are imposing on matrix multiplication is that it be associative in the
specific case of a matrix and its inverse. But this is not what happens in the scalar case;
associativity works not just for a special case of multiplication, but for all numbers.
We will forgo what the inverse of a matrix might mean and focus now on imposing the 
condition that matrix/matrix multiplication be associative -- just as multiplication is for numbers. 
Specifically, the condition we impose is:
\begin{eqnarray}
	(B\, A) {\bf x} & = & B (A \, {\bf x}) \quad \forall {\bf x} \in R^n \label{ASSOC}
\end{eqnarray}
Notice that we are imposing what matrix/matrix multiplication is by saying how the 
new matrix formed, $B \, A$, acts on an {\em arbitrary\/} vector, ${\bf x}$. And we 
specify that action by the right hand side of $(\ref{ASSOC})$, which involves only matrix/vector
multiplication -- something we already know.

We want to emphasize that this is a strong condition to impose; meaning, that the 
requirement that this is true for {\em all\/} ${\bf x}$ gives us a lot to work with.
For instance, if one had two $n\times n$ matrices, $C$ and $D$, what could you conclude if someone
told you that $C {\bf x} = D {\bf x}$ for {\em some\/} $n$ vector, ${\bf x}$?
Well, the answer is: not much.
However, if I told you that $C {\bf x} = D {\bf x}$ {\em for all\/} ${\bf x}$, what could you say?
I claim that tells us that $C \equiv D$; meaning that every entry in $C$ is the same as the corresponding
entry in $D$. We can see this by applying $C$ and $D$ to each of the ${\bf e}_i$, ($i \in [1, n]$) vectors.
For each $i$, $C {\bf e}_i$ and $D {\bf e}_i$ pick off the $i^{\rm th}$ column of 
$C$ and $D$ respectively. So we see that each column of $C$ and $D$ must match; consequently,
$C \equiv D$.

We apply this idea to $(\ref{ASSOC})$ to find a formula for matrix/matrix multiplication.
Since $(\ref{ASSOC})$ must be true {\em for all\/} ${\bf x}$ we can choose the vectors $\{\bf e\}_{i=1}^n$.
This gives us the following equations:
\begin{eqnarray}
    (B \, A) {\bf e}_j & = & B (A \, {\bf e}_j) 
\end{eqnarray}
Or,
\begin{eqnarray}
	({\bf B} \, {\bf A})^j & = & B  {\bf A}^j 
\end{eqnarray}
For any given, $i$, the $i^{\rm th}$ entry of the left and right hand side is
(after using the definition of matrix/vector multiplication, $(\ref{matrix-vector-mult})$ on the right hand side)
\begin{eqnarray}
	\left[({\bf B} \, {\bf A})^j\right]_i & = & \sum_{k=1}^n B_{i,k}  \left[{\bf A}^j\right]_k  \quad \forall i \in [1, n]
\end{eqnarray}
Using $(\ref{vec-matrix-index})$ on the left and right hand sides of the previous equation we have
\begin{eqnarray}
	(B \, A)_{i, j} & = & \sum_{k=1}^n B_{i, k}  A_{k, j}  \label{matrix-matrix-mult}
\end{eqnarray}
And we are done, we have shown what the new matrix, $B \, A$ is by showing what every entry, $i, j$ of
the new matrix is.

The next order of business is to identify an $n \times n$ matrix which serves as an
identity (in terms of matrix/vector multiplication).
Let us suppose that we have such a matrix and let's call it $I$.
Then we must have $I \, {\bf x} = {\bf x} \quad \forall {\bf x} \in R^n$. 
Then, it is not hard
to see that $I \, {\bf e}_{i} = {\bf e}_i \quad \forall i \in [1, n]$. 
However, we know that $I \, {\bf e}_i = I^i$. Therefore, $I$
must have the property that ${\bf I^i} = {\bf e}_i$. Consequently $I$ must have the form:
\begin{eqnarray}
  I & = & \left[
      \begin{array}{cccccc}
        1 & 0 & 0 & 0 & \cdots & 0 \\
        0 & 1 & 0 & 0 & \cdots & 0 \\
        0 & 0 & 1 & 0 & \cdots & 0 \\
        0 & 0 & 0 & 0 & \ddots & 0 \\
        0 & 0 & 0 & 0 & \cdots & 1 \\
      \end{array}
  \right]
\end{eqnarray}
That is $I_{ij} = \delta_{ij}$.%
\footnote{$\delta_{ij} = \left\{\begin{array}{ll}
                                 1 & \text{if $i = j$;} \\
                                 0 & \text{otherwise.}
                               \end{array} \right.$}
We have shown that the only candidate matrix that has the identity property is the
matrix $I$. That is, if there is an identity matrix, it must be $I$. Does it satisfy
the property of being an identity matrix (again, in the matrix/vector multiplication world)?
Do we have $I \, {\bf x} = {\bf x}$ for all ${\bf x} \in R^n$?
Using the definition of matrix/vector multiplication, $(\ref{matrix-vector-mult})$, we
have for any given, $i$:
\begin{eqnarray}
  \left[I {\bf x}\right]_i & = & \sum_{i=1}^n I_{ij} x_j \nonumber \\
                           & = & \sum_{i=1}^n \delta_{ij} x_j \nonumber \\
                           & = & x_i \nonumber \\
                           & = & [{\bf x}]_i
\end{eqnarray}
One can show that this identity matrix, $I$, is also the identity operator for
matrix/matrix multiplication.

The only thing left is to know when a matrix inverse exists and how to compute it.
We do not attempt to do this in this paper. In the next section we continue with
a qualitative comparison of the solution to the scalar problem, $a\, x = b$, 
and its vectorized cousin.


\section{Qualitative Features of Solutions}
We can view the multiplication of two numbers, $a$ and $x$, as just that.
Or, we can think of $a$ being fixed and letting $x$ "run-through" all 
numbers. Here we see two cases: if $a \neq 0$, then letting $x$ run
through all of the numbers in $R$ will produce all of the numbers in $R$.
We could think of $a$ as an "operator" and call the set of all possible 
outputs, the range of $a$ and denote it: ${\cal R}(a)$.
There is another case, $a$ could be zero. In this case it's range is the
set $\{0\}$. In the first case, with non-zero $a$ it is clear that we can
find an $x$ to ``hit'' a given value $b$. That is, we can solve $a\, x = b$.

One can define the same concept for a matrix, $A$. Using this language
of ranges, here is what we can say about the scalar problem: $a \, x = b$.
\begin{description}
    \item[Unique Solution:]{If $a^{-1}$ exists ($a \neq 0$), $b$ is any number (that is: $(b \in {\cal R}(a))$}
		then there is a {\bf unique\/} solution.
\item[No Solution:]{If $a^{-1}$ does not exist (i.e., $a = 0$) {\em AND\/}
	$b$ is {\bf not\/} in the range of $a$ (that is: $b \ne 0$), then there is {\bf no\/} solution.}
\item[Infinite Solutions:]{If $a^{-1}$ does not exist (i.e., $a = 0$) {\em BUT\/}
	$b$ is in the range of $a$ (that is: $b = 0$), 
	then there are an {\bf infinite\/} number of solutions.}
\end{description}

Here is the analog of this solution categorization for the multi-dimensional case: $A \, {\bf x} = {\bf b}$.
\begin{description}
	\item[Unique Solution:]{If $A^{-1}$ exists, ${\bf b\/}$ is in the range of $A$ (${\bf b} \in R(A)$)
		then there is a {\bf unique\/} solution.}
\item[No Solution:]{If $A^{-1}$ does not exist {\em AND\/}
	${\bf b\/}$ is not in the range of $A$ (that is ${\bf b} \notin {\cal R}(A)$),
then there is {\bf no\/} solution.}  
\item[Infinite Solutions:]{If $A^{-1}$ does not exist {\em BUT\/}
		${\bf b\/}$ is in the range of $A$ (that is: ${\bf b} \in {\cal R}(A)$),
then there are an {\bf infinite\/} number of solutions.} 
\end{description}

\end{document}

