\documentclass{article}
% \usepackage{fonttable,textcomp}
% \usepackage[T1]{fontenc}
% \usepackage{NewYorker}
\usepackage{amsmath,amscd}

\title{Origins of Matrix/Vector and Matrix/Matrix Multiplication}
\author{R. Scott McIntire}
\date{Aug 30, 2024}
\setlength{\textwidth}{6.5in}
\setlength{\textheight}{7.5in}
\setlength{\hoffset}{-0.75in}
\setlength{\voffset}{-0.75in}

\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.5\baselineskip}


\begin{document}
\maketitle


\section{Algebraic properties of Matrices}
Why is matrix/vector and matrix/matrix multiplication defined the way it is?
One motivation is to try to extend the (albeit trivial) solution of linear
equations in 1-dimension to n dimensions. To do this, we go through the solution
of the 1-dimensional case in careful detail.

The scalar problem is:
\begin{eqnarray}
  a \, x & = & b \label{scalar-problem}
\end{eqnarray}

Here is a very explicit solution keeping an eye towards generalization.
\begin{eqnarray}
  a \, x & = & b \\
  a^{-1} ( a \, x ) & = & a^{-1} b \quad \quad \text{(Multiply by Inverse)} \label{inv} \\
  (a^{-1} a) x & = & a^{-1} b \quad \quad \text{(Use associativity of multiplication)} \label{assoc} \\
  1 \, x & = & a^{-1} b \quad \quad \text{(What Inverse multiplication does)} \label{inv-mult} \\
  x & = & a^{-1} b \quad \quad \text{(What identity multiplication does)} \label{identity}
\end{eqnarray}

The multi-dimensional problem has a lot more variables and coefficients.
To start, we need to be more systematic about the naming of these coefficients.
With this in mind, the multi-dimensional linear problem can be written:
\begin{eqnarray*}
  a_{11} x_1 + a_{12} x_2 + \cdots + a_{1n} x_n & = & b_1 \\
  a_{21} x_1 + a_{22} x_2 + \cdots + a_{2n} x_n & = & b_2 \\
  \cdots & = & \cdots \\
  a_{n1} x_1 + a_{n2} x_2 + \cdots + a_{nn} x_n & = & b_n 
\end{eqnarray*}
Here, $a_{ij}$ is the coefficient in the $i^{\rm th}$ row and $j^{\rm th}$ column.


To make this look like the 1-dimensional case, we need to think of the $b$'s as
a single unit. Our single unit will be the {\em vector\/} of the $b$'s.

The multi-dimensional case can be rewritten in vector terms as:
\begin{eqnarray}
  \left[
  \begin{array}{c}
  a_{11} x_1 + a_{12} x_2 + \cdots + a_{1n} x_n \\
  a_{21} x_1 + a_{22} x_2 + \cdots + a_{2n} x_n \\
  \cdots  \\
    a_{n1} x_1 + a_{n2} x_2 + \cdots + a_{nn} x_n
  \end{array}
  \right] & = & \left[
                 \begin{array}{c}
                   b_1 \\
                   b_2 \\
                   \vdots \\
                   b_n
                 \end{array}
   \right] \label{vector-form}
\end{eqnarray}
Two things need to be done: treat the $x$'s as a unit -- as we did with the
$b$'s -- and separate the $a$ coefficients from the $x$'s. This must be done formally
and yet have the same meaning as the original problem formulation.
This is done in the most natural of ways:
\begin{eqnarray}
  \left[
  \begin{array}{cccc}
    a_{11} & a_{12} & \cdots & a_{1n} \\
    a_{21} & a_{22} & \cdots & a_{2n} \\
    \vdots \\
    a_{n1} & a_{n2} & \cdots & a_{nn} \\
    \end{array}
  \right]
  \left[
  \begin{array}{c}
    x_1 \\
    x_2 \\
    \vdots \\
    x_n
  \end{array}
  \right] & = &
                \left[
                \begin{array}{c}
                  b_1 \\
                  b_2 \\
                  \vdots \\
                  b_n
                \end{array}
  \right] \label{matrix-form}
\end{eqnarray}
Taking the rectangular collection (matrix) of $a$'s to be $A$; the (vector) collection
of $x$'s to be ${\bf x}$; and the collection of the $b$'s to be ${\bf b}$;
we may write $(\ref{matrix-form})$ as:
\begin{eqnarray}
  A \, {\bf x} & = & {\bf b} \label{concise-matrix-form}
\end{eqnarray}
This now looks like the scalar problem, $(\ref{scalar-problem})$.

We proceed to try solving using the solution procedure used above for the scalar case.
In the process, we will need to:
\begin{itemize}
    \item{Vectorize the input variable $x$ and the outputs $b$. -- {\bf Done}.}
    \item{Define an, $a$, matrix. -- {\bf Done}.}
    \item{Define matrix-vector multiplication.}
    \item{Define matrix-matrix multiplication.}
    \item{Define the identity matrix.}
    \item{Define inverse matrix.}
\end{itemize}

How do we make sense of this matrix/vector syntax? It should have
the proper meaning; that is, $(\ref{matrix-form})$
should have the same meaning as $(\ref{vector-form})$.

Examining $(\ref{matrix-form})$ and the left hand side of
$(\ref{vector-form})$, gives us our definition of matrix/vector multiplication:
\begin{eqnarray}
  \left[A \, {\bf x}\right]_i & \equiv & \sum_{j=1}^n A_{ij} x_j
                                         \label{matrix-vector-mult}
\end{eqnarray}
That is, $A$ acts on a vector, ${\bf x}$, to create a new vector, $A \, {\bf x}$,
whose $i^{\rm th}$ entry is the $i^{\rm th}$ entry of
the left hand side of $(\ref{vector-form})$.

Another way to write this is:
\begin{eqnarray}
  A {\bf x} & = & \sum_{i=1}^n x_i {\bf A}^i
\end{eqnarray}
Here, ${\bf A}^i$ is the $i^{\rm th}$ column of $A$. Applying $A$ to the special
vectors, ${\bf e}_i$
-- which are $0$ everywhere except at $i$ where they are $1$ --
we see that $A \, {\bf e}_i = {\bf A}^i$. Consequently, this matrix/vector
multiplication determines $A$ uniquely
-- if there is another matrix, its columns would have to match $A$.

To complete the solution outline, matrix/matrix multiplication is needed
via $(\ref{assoc})$. How must this be defined?
Well, we need to make sense of:
\begin{eqnarray}
  (A^{-1} A) {\bf x} & = & A^{-1} (A \, {\bf x}) 
\end{eqnarray}
This involves the inverse of the matrix $A$, which we also need to define. Once
we have the inverse we then need to do matrix-matrix multiplication.
Let us start with matrix-matrix multiplication.
This will be determined by the fact that we (just as in the scalar case) demand that the multiplication
be {\em associative\/}.
\footnote{We know from above that defining how a given matrix acts on all vectors --
  via matrix/vector multiplication -- uniquely determines the matrix. So, this is a proper definition of matrix/matrix multiplication.}
\begin{eqnarray}
    (B \, A) {\bf x} & \equiv & B (A \, {\bf x}) \quad \forall x \in R^n
\end{eqnarray}
This definition would mean that $B \, A$ is a new $n \times n$ matrix whose
$i^{\rm th}$ entry -- when acting on an arbitrary vector ${\bf x}$ --
is (using $(\ref{matrix-vector-mult})$ twice):
\begin{eqnarray}
    \left[(B \, A) {\bf x}\right]_i &  = & \left[B (A \, {\bf x})\right]_i = \sum_{k=1}^n B_{ik} \left(\sum_{j=1}^n A_{kj} x_j\right) \quad \forall x \in R^n, \forall i \in [1,n]
\end{eqnarray}
Using $(\ref{matrix-vector-mult})$ on the left hand side yields:
\begin{eqnarray}
  \sum_{j=1}^n (B \, A)_{ij} x_j &  = & \sum_{k=1}^n B_{ik} \left(\sum_{j=1}^n A_{kj} x_j\right)
\end{eqnarray}
Or,
\begin{eqnarray}
  \sum_{j=1}^n (B \, A)_{ij} x_j &  = & \sum_{k=1}^n \sum_{j=1}^n B_{ik} A_{kj} x_j
\end{eqnarray}
Changing the order of summation on the right hand side, this is:
\begin{eqnarray}
  \sum_{j=1}^n (B \, A)_{ij} x_j &  = & \sum_{j=1}^n \left(\sum_{k=1}^n B_{ik} A_{kj}\right) x_j \label{matrix-matrix-deriv}
\end{eqnarray}
Or,
\begin{eqnarray}
    \sum_{j=1}^n \left[ (B \, A)_{ij} - \left(\sum_{k=1}^n B_{ik} A_{kj}\right)\right] x_j & = & 0\label{matrix-matrix-deriv-2}
\end{eqnarray}
This suggests that the $i^{\rm th}, j^{\rm th}$ entry of the multiplication of $B$ and $A$ is:
\begin{eqnarray}
    (B \, A)_{ij} & = & \sum_{k=1}^n B_{ik} A_{kj} \label{matrix-matrix-mult} \quad \forall i \in R^n, \forall j \in R^n
\end{eqnarray}

To see that this follows, notice that $(\ref{matrix-matrix-deriv-2})$ must hold for all
vectors ${\bf x}$. Setting ${\bf x}$ to the successive ${\bf e}_i$ vectors defined
above yields $(\ref{matrix-matrix-mult})$.


By $(\ref{identity})$ we need to identify an $n \times n$ matrix which serves as an
identity (in terms of matrix/vector multiplication).
Let us suppose that we have such a matrix and let's call it $I$.
Then we must have $I \, {\bf x} = {\bf x} \quad \forall {\bf x} \in R^n$. 
Then, it is not hard
to see that $I \, {\bf e}_{i} = {\bf e}_i \quad \forall i \in [1, n]$. 
However, we know that $I \, {\bf e}_i = I^i$. Therefore, $I$
must have the property that ${\bf I^i} = {\bf e}_i$. Consequently $I$ must have the form:
\begin{eqnarray}
  I & = & \left[
      \begin{array}{cccccc}
        1 & 0 & 0 & 0 & \cdots & 0 \\
        0 & 1 & 0 & 0 & \cdots & 0 \\
        0 & 0 & 1 & 0 & \cdots & 0 \\
        0 & 0 & 0 & 0 & \ddots & 0 \\
        0 & 0 & 0 & 0 & \cdots & 1 \\
      \end{array}
  \right]
\end{eqnarray}
That is $I_{ij} = \delta_{ij}$.%
\footnote{$\delta_{ij} = \left\{\begin{array}{ll}
                                 1 & \text{if $i = j$;} \\
                                 0 & \text{otherwise.}
                               \end{array} \right.$}
We have shown that the only candidate matrix that has the identity property is the
matrix $I$. That is, if there is an identity matrix, it must be $I$. Does it satisfy
the property of being an identity matrix (again, in the matrix/vector multiplication world)?
Do we have $I \, {\bf x} = {\bf x}$ for all ${\bf x} \in R^n$?
Using the definition of matrix/vector multiplication, $(\ref{matrix-vector-mult})$, we
have for any given, $i$:
\begin{eqnarray}
  \left[I {\bf x}\right]_i & = & \sum_{i=1}^n I_{ij} x_j \nonumber \\
                           & = & \sum_{i=1}^n \delta_{ij} x_j \nonumber \\
                           & = & x_i \nonumber \\
                           & = & [{\bf x}]_i
\end{eqnarray}
One can show that this identity matrix, $I$, is also the identity operator for
matrix/matrix multiplication.

The only thing left is to know when a matrix inverse exists and how to compute it.
We do not attempt to do this in this paper. In the next section we continue with
a qualitative comparison of the solution to the scalar problem, $a\, x = b$, 
and its vectorized cousin.


\section{Qualitative Features of Solutions}
We can view the multiplication of two numbers, $a$ and $x$, as just that.
Or, we can think of $a$ being fixed and letting $x$ "run-through" all 
numbers. Here we see two cases: if $a \neq 0$, then letting $x$ run
through all of the numbers in $R$ will produce all of the numbers in $R$.
We could think of $a$ as an "operator" and call the set of all possible 
outputs, the range of $a$ and denote it: ${\cal R}(a)$.
There is another case, $a$ could be zero. In this case it's range is the
set $\{0\}$. In the first case, with non-zero $a$ it is clear that we can
find an $x$ to ``hit'' a given value $b$. That is, we can solve $a\, x = b$.

One can define the same concept for a matrix, $A$. Using this language
of ranges, here is what we can say about the scalar problem: $a \, x = b$.
\begin{description}
    \item[Unique Solution:]{If $a^{-1}$ exists ($a \neq 0$), $b$ is any number (that is: $(b \in {\cal R}(a))$}
		then there is a {\bf unique\/} solution.
\item[No Solution:]{If $a^{-1}$ does not exist (i.e., $a = 0$) {\em AND\/}
	$b$ is {\bf not\/} in the range of $a$ (that is: $b \ne 0$), then there is {\bf no\/} solution.}
\item[Infinite Solutions:]{If $a^{-1}$ does not exist (i.e., $a = 0$) {\em BUT\/}
	$b$ is in the range of $a$ (that is: $b = 0$), 
	then there are an {\bf infinite\/} number of solutions.}
\end{description}

Here is the analog of this solution categorization for the multi-dimensional case: $A \, {\bf x} = {\bf b}$.
\begin{description}
	\item[Unique Solution:]{If $A^{-1}$ exists, ${\bf b\/}$ is in the range of $A$ (${\bf b} \in R(A)$)
		then there is a {\bf unique\/} solution.}
\item[No Solution:]{If $A^{-1}$ does not exist {\em AND\/}
	${\bf b\/}$ is not in the range of $A$ (that is ${\bf b} \notin {\cal R}(A)$),
then there is {\bf no\/} solution.}  
\item[Infinite Solutions:]{If $A^{-1}$ does not exist {\em BUT\/}
		${\bf b\/}$ is in the range of $A$ (that is: ${\bf b} \in {\cal R}(A)$),
then there are an {\bf infinite\/} number of solutions.} 
\end{description}

\end{document}

