\documentclass{article}

\usepackage{amsmath}


\setlength{\textwidth}{6.5in}
\setlength{\textheight}{7.5in}
\setlength{\hoffset}{-0.75in}
\setlength{\voffset}{-0.75in}
\setlength{\parindent}{0pt}
\setlength{\parskip}{0.5\baselineskip}

\title{Derivation of the Exponential Moving Average}
\author{R. Scott McIntire}
\date{Aug 26, 2023}

\begin{document}

\maketitle


\section{Overview}

The essence of the {\em exponential moving average\/} (EMA) is to provide 
a {\em localized\/} estimate of a time series as it proceeds. It does 
this by using a weighted average of the values in a "window" around each point 
of the series in way that each summand in 
the average is multiplied by powers of a factor, $\lambda$,
in the interval $(0,1)$.
In what follows below we will assume a 1-based index, and that 
the data series ${\bf x}$ has length $N$.

Specifically, the exponential moving average (over a window of length $m$) 
is defined as:
\begin{eqnarray}
    e_n & = & \sum_{k=0}^{m-1} x_{n-k} w_{k+1} \label{ema_def}
\end{eqnarray}
To get the {\em exponential decay\/} in weights we assume that $w_{k+1} = \lambda w_k$.%
\footnote{The reason this is called exponential is that it is the discrete
analog of continuous exponential decay. To see this let $f(x) = e^{-kt}$ 
for $k > 0$. Then sampling this function at regular intervals of say $1$ 
we get the sequence: $\{ e^{-k}, e^{-2k}, e^{-3k}, \ldots \}$.
Here we can see that the elements of the sequence are powers of a value, $\lambda = e^{-k}$.
That is, the sequence is nothing other than: $\{\lambda, \lambda^2, \lambda^3, \ldots \}$.}
Since the parameter $\lambda$ is in the interval $(0,1)$, it represents a decay factor. 
Notice that the indices of ${\bf x}$ and that of ${\bf w}$ go in the "opposite direction".
This is done so that the largest weights are associated with the most recent data
in the averaging window.

Before we go further, there is a problem with the formula as stated.
If $n = 1$; or, in fact any value that is less than or equal to $m$,
the expression $x_{n-k}$ does not make sense for $k \ge n$. That is, at the 
start of the computation and for a little while, there is not a full window
of values over a window of length $m$. As there is no information before $x_1$,
we interpret these values to be $x_1$. 
We will apply this principal more generally below. 
That is, if ${\bf z}$ is a data series,
we extend the definition of ${\bf z}$ so that it is defined for
indices less than 1 as:

\begin{eqnarray}
 z_{k} & = & \begin{cases}
     z_{k} & k \ge 1; \cr
     z_1 & \text{otherwise}. \cr
 \end{cases} \label{o:var_extend}
\end{eqnarray}

While we can use $(\ref{ema_def})$ to compute the EMA it is {\em inefficient\/}. 
The purpose of this
paper is to derive a recursive formula that is fast to compute.
The formula takes advantage of the fact that when the weights are exponentials
(powers of $\lambda$) we don't have to recompute the bulk of the sums in the 
averaging window.

Regardless of the averaging scheme, we want our weights to sum to one.%
\footnote{If we consider the underlying data to be generated from a series of identically distributed random
variables; then, treating the series elements as random variables we can view 
the moving average as an estimate of their mean, $\mu$. 
To be a good estimate we which this estimate to be {\em unbiased\/}:
$E\left[\sum_{k=1}^m x_{n-k} w_{k+1}\right] = \sum_{k=0}^{m-1} E[x_{n-k}]w_{k+1}$.
Which is $\mu \sum_{k=0}^{m-1} w_{k+1} = \mu \sum_{k=1}^{m} w_k = \mu$.}
But we also want the weights to be exponential; that is, $w_{k+1} = \lambda w_{k}$.

Ideally, all we want to do is give the decay factor, $\lambda$. If so, 
how do we determine the weights?
The weights can be constructed as follows once a $\lambda$ is given.

For $i \in [1, m]$ define
\begin{eqnarray}
    {\hat w}_i & = & \lambda^i 
\end{eqnarray}
The ${\hat {\bf w}}$ have the property that they decay with $\lambda$;
that is, $\lambda {\hat w}_{k} = {\hat w}_{k+1}$. However, how do we ensure that
these values sum to 1 over the window? 

To do this, we simply normalize the weights:
\begin{eqnarray}
w_i & = &  \frac{{\hat w}_i}{\sum_{k=1}^m {\hat w}_k} \quad \forall i \in [1, m]
\end{eqnarray}
Consequently,
\begin{eqnarray}
    \sum_{i=1}^m w_i & = & 1 \label{o:weights_normalized}
\end{eqnarray}

In doing this, did the proposed weights, ${\bf w}$, lose the decay factor
property? The answer is no, the property remains: 
\begin{eqnarray}
    \lambda w_{i} = w_{i+1} \quad \forall i \in [1, m-1]. \label{o:weights_power}
\end{eqnarray}

{\bf Note:\/} We will $(\ref{o:var_extend})$, $(\ref{o:weights_normalized})$,
and $(\ref{o:weights_power})$ in our derivations below.

\section{Recursive Formula for EMA}
We state again the definition of the exponential moving average of $x$ as:
\begin{eqnarray}
e_n & = & \sum_{k=0}^{m-1} x_{n-k} w_{k+1}
\end{eqnarray}
In order to find a recursive formula, we examine the next element in the smoothing sequence 
and try to relate it to the present element at index $n$.

\begin{eqnarray*}
e_{n+1} & = & \sum_{k=0}^{m-1} x_{n+1-k} w_{k+1} \\
        & = & \sum_{k=0}^{m-1} x_{n-(k-1)} w_{k+1} \\
        & = & \sum_{k=-1}^{m-2} x_{n-k} w_{k+2}  \\
        & = & \sum_{k=0}^{m-2} x_{n-k} w_{k+2}  + x_{n+1} w_1 \\
        & = & \lambda \sum_{k=0}^{m-2} x_{n-k} w_{k+1}  + x_{n+1} w_1  \quad \text{(Since $\lambda w_{k+1} = w_{k+2}$)} \\
        & = & \lambda \sum_{k=0}^{m-1} x_{n-k} w_{k+1}  + x_{n+1} w_1  
                - \lambda x_{n-(m-1)} w_m  \\
        & = & \lambda \sum_{k=0}^{m-1} x_{n-k} w_{k+1}  + x_{n+1} w_1  
                - \lambda x_{(n+1)-m} w_m 
\end{eqnarray*}

Finally, we may write%
\footnote{From the definition of $e_n$ and using 
$(\ref{o:var_extend})$ and $(\ref{o:weights_normalized})$,
$e_1 = \sum_{k=0}^{m-1} x_{n-k} w_{k+1} = x_1 \sum_{k=0}^{m-1} w_{k+1} = x_1 \sum_{k=1}^m w_k = x_1$.}
\begin{eqnarray}
    e_{n+1} & = & \lambda \left(e_n - x_{(n+1) - m} w_m \right) + x_{n+1} w_1  \quad \forall n \in [1, N-1] \\
    e_1     & = & x_1
\end{eqnarray}

The corresponding 0-based index formula is:%
\footnote{We write the formulas down for both 1 and  0 based vector indices because
computer languages typically use one or the other.}
\begin{eqnarray}
    e_{n} & = & \lambda \left( e_{n-1} - x_{(n-m} w_{m-1} \right) + x_{n} w_0  \quad \forall n \in [1, N-1] \\
    e_0     & = & x_0
\end{eqnarray}

\section{Recursive Formula for the Std of EMA}
In practice, one also wants to know a localized standard deviation of the process.
In applications, one can provide "Bollinger" bands around the EMA estimate.
These bands are some multiple of the standard deviation of the process.

We derive the standard deviation of the exponential moving average starting
with the definition of the exponential moving average and its moving variation.%
\footnote{This variance formula needs to be corrected by a factor which we 
leave to the end. The correction is a factor that is used to make the 
estimated standard deviation of the moving average {\em unbiased\/}.
The formula for this factor is independent of the structure of the weights.
This means that it can be pre-computed independently of the data series.}

\begin{eqnarray}
    e_n & = & \sum_{k=0}^{m-1} x_{n-k} w_{k+1} \label{ema_cmp} \\
    v_n & = & \sum_{k=0}^{m-1} (x_{n-k} - e_{n-k})^2 w_{k+1}  \label{var} 
\end{eqnarray}

Note, that the form of the two equations is identical; that is, 
they both have the form:
\begin{eqnarray}
    g_n & = & \sum_{k=0}^{m-1} f(x_{n-k}) w_{k+1}
\end{eqnarray}
One can find a recursive formula $g_n$ in {\em exactly\/} the same was as
was done for $(\ref{ema_cmp})$. This is done by reworking the derivation of 
the recursive formula for $(\ref{ema_cmp})$, replacing $x_j$ with $f(x_j)$.

The recursive formula for $g_n$ is
\begin{eqnarray}
    g_{n+1} & = & \lambda \left( g_{n} - f(x_{(n+1)-m}) w_m \right) + f(x_{n+1}) w_1  \quad \forall n \in [1, N-1] \\
    g_1 & = & f(x_1)
\end{eqnarray}
The 0-based index formula is:

\begin{eqnarray}
    g_{n} & = & \lambda \left( g_{n-1} - f(x_{n-m}) w_{m-1} \right) + f(x_{n}) w_0  \quad \forall n \in [1, N-1] \\
    g_0 & = & f(x_0)
\end{eqnarray}


The corresponding $f$ for $(\ref{var})$ is $f(x) = x^2$; 
consequently, its recursive formula is 

\begin{eqnarray}
    v_{n+1} & = & \lambda \left( v_n -  ( x_{(n+1)-m} - e_{(n+1)-m} )^2 w_m \right) + ( x_{n+1} - e_{n+1} )^2 w_1 \quad \forall n \in [1, N-1] \\
    v_1  & = & (x_1 - e_1)^2 = 0
\end{eqnarray}
The corresponding 0-based index formula is:

\begin{eqnarray}
    v_{n} & = & \lambda \left( v_{n-1} - ( x_{n-m} - e_{n-m} )^2 w_{m-1}\right) + ( x_{n} - e_{n} )^2 w_0 \quad \forall n \in [1, N-1] \\
    v_0  & = & (x_0 - e_0)^2 = 0
\end{eqnarray}

Finally, the standard deviation of the exponential moving average is:%
\footnote{The window length, $m$, must be greater than $1$.}

\begin{eqnarray}
    s_{n} = \sqrt{\frac{v_n}{1 - \sum_{i=1}^m w_i^2}} 
\end{eqnarray}
The variance corrective factor\footnote{The 0-based factor formula is: $1 - \sum_{i=0}^{m-1} w_i^2$.}, $1 - \sum_{i=1}^m w_i^2$,
is a one time calculation independent of $n$. This factor is the standard 
correction when determining empirical variance of a weighted average.

{\bf Note:\/} In practice, one may wish to use some prior estimate of the standard deviation of the process
and smoothly move from that to the estimate above over the averaging windows when $n \in [1, m]$.


\section{How to Specify the $\lambda$ Factor}
When using the EMA, practitioners do not specify the factor $\lambda$ directly; 
instead they provide the ``half-life'' of the decay.
By the half-file of $\lambda$, we mean the number of powers of $\lambda$, $N$, so that
$\lambda^N \approx \frac{1}{2}$. 

In our case we are interested in the reverse; namely, given a half-life, $N$, 
determine the factor $\lambda$. To do this we solve the equation $\lambda^N = \frac{1}{2}$
for $\lambda$.
Taking the $\log$ of this equation and solving for $\lambda$ we have:%
\footnote{The log used below is the logarithm base $e$.}

\begin{eqnarray*}
    \log(\lambda^N) & = & \log(\frac{1}{2}) \\
    N \log(\lambda) & = & \log(1) - \log(2) \\
    \log(\lambda) & = & -\frac{\log(2)}{N}
\end{eqnarray*}
The formula to determine $\lambda$ given the half-life $N$ is:

\begin{eqnarray}
    \lambda = e^{-\frac{\log(2)}{N}}
\end{eqnarray}

\appendix

\section{Unbiased Estimator for a Sequence of IID Random Variables}
In this section we derive the corrective formula for computing the empirical variance.

In what follows we assume that we are given a sequence of independent identically distributed random variables (IID):
$\{x_i\}_{i=1}^N$ and corresponding weights: $\{w_i\}_{i=1}^N$.
We assume that the random variables have a mean and a variance:
\begin{eqnarray}
    E[x_i] & = & \mu  \quad \forall i, i \in [1, N] \\
    E[(x_i - \mu)]^2 & = & \sigma^2 \quad \forall i, i \in [1, N] 
\end{eqnarray}

We assume that the weights are non-negative and are normalized:%
\footnote{We also assume that the weight is not concentrated at any
one index.}
\begin{itemize}
    \item{$\sum_{i=1}^N w_i = 1$}
    \item{$\forall i, 0 \le w_i < 1$}
\end{itemize}

If we take the expectation of the weighted sum, $\sum_{i=1}^N x_i w_i$ we find that it is $\mu$:%
\footnote{This is due to the linearity of the expectation operator, $E$:
$E[a x + b y + c] = a E[x] + b E[y] + c$, provided $a,b,c$ are constants.}

\begin{eqnarray}
    E\left[\sum_{i=1}^n x_i w_i\right] = \sum_{i=1}^N E[x_i n_i ] = \sum_{i=1}^N E[x_i] w_i = \mu \sum_{i=1}^N w_i = \mu
\end{eqnarray}
The weighted sum represents an estimate of the true mean of the IID random variables, which is $\mu$.
And in this case we see that it is a good estimate in that the expected value of the weighed combination
of the random variables is $\mu$. 

We refer to such "good" estimates as {\em unbiased estimators\/}.

Now consider the sum $\sum_{i=1}^N (x_i - \mu)^2 w_i$.
People use this as an estimate of the variance of the random variables. Is this "good" (unbiased) estimate?
Yes, it is as: $E\left[ \sum_{i=1}^N (x_i - \mu)^2 w_i \right] = \sum_{i=1}^N E[(x_i- \mu)^2] w_i = \sigma^2 \sum_{i=1}^N w_i = \sigma^2$

But what if we replace the true mean in this sum with our estimate: ${\bar x} = \sum_{i=1}^N x_i w_i$?
Then it turns out that the resulting estimate for the variance is {\em biased\/}.
However, it happens that we are "off" by a certain factor and that factor depends only on the weights 
we use. In fact the factor doesn't depend on the ordering of the weights.

To find this factor we examine the expected value of the straight forward formula one would imagine
using to estimate the variance of the random variables.

\begin{eqnarray}
    E\left[ \sum_{i=1}^N (x_i - {\bar x})^2 w_i \right] & = & \sum_{i=1}^N E\left[(x_i - {\bar x})^2\right] w_i \nonumber \\
    & = & \sum_{i=1}^N E\left[\left((x_i - \mu) + (\mu - {\bar x})\right)^2\right] w_i \nonumber \\
    & = & \sum_{i=1}^N E\left[(x_i - \mu)^2 - 2(x_i - \mu)({\bar x} - \mu ) + ({\bar x} - \mu)^2\right] w_i \nonumber \\
    & = & \sum_{i=1}^N \left( E\left[(x_i - \mu)^2\right] + E\left[ ({\bar x} - \mu)^2\right] 
              - 2E\left[(x_i - \mu) ({\bar x} - \mu) \right] \right) w_i \label{standard_emp_var}
\end{eqnarray}
We separately compute the expectation of each of the three summands.
\begin{eqnarray}
    E\left[(x_i - \mu)^2\right] & = & \sigma^2
\end{eqnarray}

\begin{eqnarray}
    E\left[ ({\bar x} - \mu)^2 \right] & = & E \left[\left( \left(\sum_{j=1}^N x_j w_j\right) - \mu \right)^2\right] \nonumber \\
     & = &  E\left[ \left( \sum_{j=1}^N (x_j w_j - \mu w_j) \right)^2\right] \nonumber \\
     & = &  E\left[ \left( \sum_{j=1}^N (x_j - \mu)w_j \right)^2\right] \nonumber \\
     & = &  E\left[ \left( \sum_{j=1}^N (x_j - \mu)w_j \right) \left(\sum_{k=1}^N (x_k - \mu) w_k\right)\right] \nonumber \\
     & = &  E\left[ \sum_{j=1}^N \sum_{k=1}^N  (x_j - \mu) (x_k - \mu) w_j w_k \right]  \nonumber \\
     & = &  \left( \sum_{j=1}^N \sum_{k=1}^N  E\left[(x_j - \mu) (x_k - \mu) \right]w_j w_k\right) \nonumber  \\
     & = &  \left( \sum_{j=1}^N \sum_{k=1}^N  \sigma^2 w_j w_k \delta_{j,k}\right) \nonumber  \\
     & = &  \left( \sum_{j=1}^N \sigma^2 w_j^2 \right)  \nonumber \\
     & = &  \sigma^2 \left( \sum_{j=1}^N w_j^2 \right)  
\end{eqnarray}

\begin{eqnarray}
    E\left[(x_i - \mu)({\bar x} - \mu)\right]  & = & E\left[ (x_i - \mu) \left( \left( \sum_{j=1}^N x_j w_j\right) - \mu \right)  \right] \nonumber \\ 
    & = & E\left[ (x_i - \mu) \left( \sum_{j=1}^N (x_j - \mu)w_j\right) \right] \nonumber \\ 
    & = & E\left[ \sum_{j=1}^N (x_i - \mu) (x_j - \mu)w_j \right] \nonumber \\ 
    & = & \sum_{j=1}^N E\left[(x_i - \mu) (x_j - \mu) \right] w_j \nonumber \\ 
    & = & \sum_{j=1}^N \sigma^2 w_j \delta_{i,j} \nonumber \\ 
    & = & \sigma^2 w_i  \nonumber 
\end{eqnarray}

Continuing with $(\ref{standard_emp_var})$ we have

\begin{eqnarray*}
    E\left[ \sum_{i=1}^N (x_i - {\bar x})^2 w_i \right] & = & \sum_{i=1}^N E\left[(x_i - {\bar x})^2\right] w_i \\
    & = & \sum_{i=1}^N E\left[\left((x_i - \mu) + (\mu - {\bar x})\right)^2\right] w_i \\
    & = & \sum_{i=1}^N E\left[(x_i - \mu)^2 - 2(x_i - \mu)({\bar x} - \mu ) + ({\bar x} - \mu)^2\right] w_i \\
    & = & \sum_{i=1}^N \left( E\left[(x_i - \mu)^2\right]  
    + E\left[\left(\sum_{j=1}^N (x_j - \mu)w_j\right)^2\right]  
              - 2E\left[(x_i - \mu) \left(\sum_{j=1}^N (x_j - \mu)w_j\right)  \right] \right)  w_i \\
    & = & \sum_{i=1}^N \left( E\left[(x_i - \mu)^2\right]  
    + \left(\sum_{j=1}^N E[(x_j - \mu)^2]w_j^2 \right)  
              - 2E\left[(x_i - \mu) \left(\sum_{j=1}^N (x_j - \mu)w_j\right)  \right] \right)  w_i \\
    & = & \sum_{i=1}^N \left(\sigma^2  + \sigma^2 \sum_{j=1}^N w_j^2 - 2 \left( \sum_{j=1}^N E[(x_i - \mu) (x_j - \mu)] w_j \right)\right)w_i  \\
    & = & \sum_{i=1}^N \left(\sigma^2  + \sigma^2 \sum_{j=1}^N w_j^2 - 2 \left( \sum_{j=1}^N \sigma^2 \delta_{i,j} w_j \right)\right)w_i  \\
    & = & \sigma^2 \sum_{i=1}^N \left(1  + \sum_{j=1}^N w_j^2 - 2 w_i \right)w_i  \\
    & = & \sigma^2 + \sigma^2 \left( \sum_{j=1}^N w_j^2 \right) - 2\sigma^2 \left( \sum_{i=1}^N w_i^2 \right) \\ 
    & = & \sigma^2\left(1 - \sum_{i=1}^N w_i^2\right) 
\end{eqnarray*}
Therefore, to make $\sum_{i=1}^N (x - {\hat x})^2 w_i$ an unbiased estimator of the variance $\sigma^2$ 
we need to correct by dividing by the factor $\left( 1 - \sum_{i=1}^N w_i^2 \right)$.

Consequently, $\frac{1}{1 - \sum_{i=1}^N w_i^2} \sum_{i=1}^N (x_i - {\hat x})^2 w_i$ 
is an unbiased estimator for the variance of any of the IID variables: $\{x_i\}_{i=1}^N$. 
The unbiased standard deviation is then
\begin{eqnarray}
    \sqrt{\frac{1}{1 - \sum_{i=1}^N w_i^2} \sum_{i=1}^N (x_i - {\bar x})^2 w_i} 
\end{eqnarray}

What is the formula when the weights are evenly distributed? That would mean that the weights are $w_i = \frac{1}{N}$.
In this case the estimate for the standard deviation of the random variables is:
\begin{eqnarray*}
     \sqrt{\frac{1}{1 - \sum_{i=1}^N \left(\frac{1}{N}\right)^2} \sum_{i=1}^N (x_i - {\bar x})^2 \frac{1}{N}} 
                   & = & \sqrt{\frac{1}{1 - \frac{1}{N}} \frac{1}{N} \sum_{i=1}^N (x_i - {\bar x})^2 } \\
                   & = & \sqrt{\frac{1}{\frac{N - 1}{N}} \frac{1}{N} \sum_{i=1}^N (x_i - {\bar x})^2 } \\
                   & = & \sqrt{\frac{N}{N - 1} \frac{1}{N} \sum_{i=1}^N (x_i - {\bar x})^2 } \\
                   & = & \sqrt{\frac{1}{N - 1} \sum_{i=1}^N (x_i - {\bar x})^2 }
\end{eqnarray*}
We see that we end up with the usual "sample" standard deviation.

\end{document}


