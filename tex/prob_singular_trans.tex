%% My macros.
\input macro

\parindent=0pt
\parskip=.5\baselineskip
\baselineskip = 1.1\baselineskip

\footline{\hss\tenrm\folio\hss}

% Step 1: Load the bold math italic font
\font\tenbmi=cmmib10

% Step 2: Allocate a new math family
\newfam\bmitfam

% Step 3: Assign the font to the new math family
\textfont\bmitfam=\tenbmi
\scriptfont\bmitfam=\tenbmi
\scriptscriptfont\bmitfam=\tenbmi

% Step 4: Define the bold mu symbol
\mathchardef\boldmu="116\bmitfam

\mantitle{Singular Transformations of Probability Density Functions}
{R. Scott McIntire} {Dec 3, 2024}

\subsection{Overview}
For non-singular transformations of variables with distributions, there 
are standard formulas that describe the probability distribution 
of the target variable. However, in the case of singular transformations,
there is no immediate formula. Often what is done is to add independent 
variables in a somewhat ad hoc fashion to make the transformation 
non-singular, and then ``integrate out'' -- if possible -- 
the independent variables from the resulting formula. 
Below we describe a procedure to determine the
target distribution with a more systematic process.

\subsection{Singular Transformation Density Formula}
We provide and prove a theorem for constructing the probability 
density function of a random variable $Y$ which is a singular function of a 
random variable $X$. By singular, we mean that $Y = f(X)$ with $f$ a function
such that $f: R^n \mapsto R^m$ and $m < n$. 

\proclaim Theorem 1. Let $f : {\cal D} \subset {\bf R}^n \mapsto {\bf R}^m$ be a continuously
          differentiable function with $n > m$. In addition,
          $Df({\bf x})$ is of rank $m$ $\forall {\bf x} \in {\cal D}$.
          If for each
          ${\bf y}$, $f^{-1}({\bf y})$ is a union of $k({\bf y})$ 
          disjoint $C^1$ $n-m$ dimensional manifolds with
          parameterizations ${\bf x}_i(\mu ; {\bf y}), \;
          i\in[1,k({\bf y})], \; \mu \in \Omega_i \subset {\bf R}^{n-m}$,
          with $P_X({\bf x})$ being the continuous probability
          density function of the random variable $X$, then the 
          density function $P_Y({\bf y})$ of the random variable $Y$ 
          defined by $Y = f(X)$ exists, is continuous, and is given by

$$
\eqalignno{
P_Y({\bf y}) & = \sum_{i=1}^{k({\bf y})} \int_{\Omega_i}
{P_X({\bf x}_i({\bf y}; {\bf \mu})) \over \sqrt{\left|Df({\bf x}_i({\bf y}; \mu)) \;
Df^T({\bf x}_i({\bf y}; \mu))\right|}}
\; \sqrt{\left|D_{\mu} {\bf x}_i^T({\bf y}; \mu) \; D_{\mu}{\bf x}_i({\bf y}; \mu)\right|}  
\; d \mu & (1) \cr
}
$$

\smallskip

\subsection{Prerequisite Results}
We recall how to compute the pseudo inverse of a matrix which maps from 
a high dimension to a low dimension.

\proclaim Lemma 1. Let $A : {\bf R}^n \mapsto {\bf R}^m$ be a linear 
transformation with $n > m$. If the rank of $A$ is $m$, then the pseudo 
inverse of $A$ is $A^T (AA^T)^{-1}$.

${\bf proof:}$ A given ${\bf x} \in R^n$ can be written uniquely 
as ${\bf x} = {\bf x_1} + {\bf x_2}$ 
where ${\bf x_1} \in R(A^T)$ and ${\bf x_2} \in N(A)$. For a given 
${\bf y} \in R^m$, it easy to show that there is a unique 
${\bf x_p} \in R(A^T)$ such that 
$A {\bf x_p} = {\bf y}$. The set of all ${\bf x} \in R^n$ such that 
$A{\bf x} = {\bf y}$ has the form 
$\{ {\bf x_p + x_n} | {\bf x_n} \in N(A)\}$. The pseudo inverse of ${\bf y}$
is defined to be ${\bf x_p}$. Since $R(A^T) \perp N(A)$, ${\bf x_p}$ is the 
solution to the problem:
$$
\eqalignno{
& \min_{\bf x} \|{\bf x}\|^2 & \cr
{\rm subject \; to:} \enspace & {\bf y} = A{\bf x} & \cr
}
$$

The solution of this can be found using Lagrange multipliers. The resulting
minimization problem is :
$$
\min_{\bf x} F({\bf x}) = \min_{\bf x} \|{\bf x}\|^2 + \langle{\bf \lambda}, {\bf y} - A{\bf x\rangle}
$$
At a minimum, ${\bf x}^*$, $DF({\bf x}^*) ({\bf h}) = {\bf 0}$. 
$DF({\bf x}^*)$ 
can be computed from 
$F({\bf x} + {\bf h}) = F({\bf x}) + DF({\bf x})(h) + o({\bf h})$. The order 
${\bf h}$ terms are:
$$
2\langle{\bf x}, {\bf h}\rangle - \langle{\bf \lambda}, A{\bf h}\rangle
$$
Therefore, $DF({\bf x}^*)({\bf h}) = 
\langle 2 {\bf x}^* - A^T {\bf \lambda}, {\bf h}\rangle$. Since $DF({\bf x}^*) \equiv 0$, we
have that $2 {\bf x}^* - A^T {\bf \lambda} = {\bf 0}$. Applying $A$ to this 
last equation and using the fact that $A$ has rank $m$ and  
$A{\bf x}^* = {\bf y}$ yields:%
\ft 1 
{Since $A$ has rank $m$ (${\rm dim}(R(A)) = m$) and $R(AA^T) = R(A)$, $AA^T$ is invertible.}
${\bf \lambda} = 2 (A A^T)^{-1} {\bf y}$.
This implies that ${\bf x}^* = A^T \left(A A^T\right)^{-1} {\bf y}$.

\bigskip

We need the following change of volume formula:

\proclaim Lemma 2. Let 
$A : {\bf R}^n \mapsto {\bf R}^m$ be a linear 
transformation with $n < m$, then if $A$ has rank $n$ it transforms the 
$n$ dimensional unit cube in $R^n$ to a region in $R^m$ with 
$n$ dimensional volume 
$\sqrt{ |A^T A|}$.

${\bf proof:}$ Since $R(A)$ has rank $n$, there is a linear map 
$P: R(A) \mapsto R^n$ which is a linear isomorphism that preserves the 
inner product. That is, $\langle P {\bf x}, P{\bf y}\rangle \; = \; \langle{\bf x}, {\bf y}\rangle$. Or,
$\langle(P^TP  - I){\bf x}, {\bf y}\rangle \; = \; 0 \enspace \forall {\bf x}, {\bf y} \in R^n$. 
This implies that $P^TP \equiv I$. The change in volume of the 
unit cube under $PA$ is the same as $A$ as P preserves volumes. However,
$PA$ is a map from $R^n$ to $R^n$, so it changes the volume of the unit cube
by $|PA|$. Since for given transformations, $B, C: R^n \mapsto R^n$ we have 
$|B^T| = B$ and $|BC| = |B| |C|$, it follows that 
$|PA|^2 = |PA| |PA| = |A^TP^T| |PA| = |A^TP^T P A| = |A^TA|$. The last 
equality comes from the fact that $P^TP = I$.
Therefore, the change in volume is $\sqrt{|A^TA|}$.

\bigskip

\subsection{Proof of Singular Transformation Formula}

We now proceed with the proof of Theorem 1.
\smallskip

${\bf proof:}$ Let ${\bf \mu}$ be the induced probability measure defined by%
\ft 2{The measure extends in a natural way to all Lebesgue measurable sets.}
$$
{\bf \mu}(\Omega_y) = \int_{f^{-1}(\Omega_y)} P_X({\bf x}) \, d{\bf x}
$$

Given an arbitrary Borel measurable region $\Omega_Y$ and letting
$\Omega_X = f^{-1}(\Omega_Y)$ we have that
$$
\eqalignno{
{\bf \mu}(\Omega_Y) & = 
\int_{\Omega_X} P_X({\bf x}) \, d{\bf x} & (1) \cr
}
$$
{\it Assume\/} that $\Omega_X$ is composed of just one $n-m$ dimensional
manifold. In this case there are local coordinates ${\bf x_1}, {\bf x_2}$, 
and manifolds $\Omega_{X_1}$, $\Omega_{X_2}$, such that the     manifold, 
$\Omega_X$ is a product of $\Omega_{X_1}$ and $\Omega_{X_2}$ with
the component manifolds $\Omega_{X_1}$, $\Omega_{X_2}$, 
locally tangent to $R(Df^T({\bf x}))$ and $N(Df({\bf x}))$ respectively.
Therefore,
$$
{\bf \mu}(\Omega_y) = 
\int_{\Omega_X} P_X({\bf x}({\bf x_1}, {\bf x_2})) \, 
d{\bf x_1} \wedge d{\bf x_2}
$$
Since for a given transformation $A$, $R(A^T) \perp N(A)$, it follows
that the product measure $d{\bf x_1} \wedge d{\bf x_2}$ is equal to 
the product of the component measures.
Thus, we may write (1) as 
$$
{\bf \mu}(\Omega_y) = 
\int_{\Omega_{X_1}} \int_{\Omega_{X_2}} 
P_X({\bf x}({\bf x_1}, {\bf x_2})) \, d{\bf x_1} \, d{\bf x_2}
$$
For a given ${\bf x_2}$, ${\bf y}$ maps to the manifold $\Omega_{X_1}$ in an 
invertible way (locally). The linear approximation to the map $f$ is $Df$, its
pseudo inverse is by lemma 1:
${\bf x} = Df^T({\bf x}) (Df({\bf x}) Df^T({\bf x}))^{-1} {\bf y}$.
Using ${\bf y}$ to express ${\bf x_1}$ combined with lemma 2 we have
$$
{\bf \mu}(\Omega_y) = 
\int_{\Omega_Y} \int_{\Omega_{X_2}} 
{P_X({\bf x}({\bf y}, {\bf x_2})) \over 
\sqrt{ |Df({\bf x}({\bf y}, {\bf x_2})) Df^T({\bf x}({\bf y}, {\bf x_2}))| } }
\, d{\bf x_2} \, d{\bf y}
$$
We argue now that the measure ${\bf \mu}$ is absolutely continuous with 
respect to Lebesgue measure%
\ft 1 {Since the Lebesgue measure is regular, any Lebesgue measurable set can be approximated by a Borel measurable set.} 
so that there exists a measurable function, $P_Y(y)$
such that $\mu(\Omega_Y) = \int_{\Omega_Y} P_Y(y) \, d{\bf y}$.
So that
$$
\int_{\Omega_Y} P_Y({\bf y})\, d{\bf y} =
\int_{\Omega_Y} \int_{\Omega_{X_2}} 
{P_X({\bf x}({\bf y}, {\bf x_2})) \over 
\sqrt{ |Df({\bf x}({\bf y}, {\bf x_2})) Df^T({\bf x}({\bf y}, {\bf x_2}))| } }
\, d{\bf x_2} \, d{\bf y}
$$
Or,
$$
\int_{\Omega_Y} \left( P_Y({\bf y}) - 
 \int_{\Omega_{X_2}} 
{P_X({\bf x}({\bf y}, {\bf x_2})) \over 
\sqrt{ |Df({\bf x}({\bf y}, {\bf x_2})) Df^T({\bf x}({\bf y}, {\bf x_2}))| } }
\, d{\bf x_2} \right) \, d{\bf y} = 0
$$

Since this is true for all Borel measurable sets, $\Omega_Y$, we have
$$
\eqalignno{
P_Y({\bf y}) & =
\int_{\Omega_{X_2}} 
{P_X({\bf x}({\bf y}, {\bf x_2})) \over 
\sqrt{ |Df({\bf x}({\bf y}, {\bf x_2})) Df^T({\bf x}({\bf y}, {\bf x_2}))| } }
\,  d{\bf x_2} & (2) \cr
}
$$
But in this case the component manifold, $\Omega_{X_2}$, is just $f^{-1}({\bf y})$.
If we parameterize this as ${\bf x}({\bf y}, {\bf \mu})$, then by lemma 2 we
have
$$
\eqalignno{
P_Y({\bf y}) =
\int_{f^{-1}(y)}
{P_X({\bf x}({\bf y}, {\bf \mu})) \over 
\sqrt{ |Df({\bf x}({\bf y}, {\bf \mu})) Df^T({\bf x}({\bf y}, {\bf \mu})) | } }
\,  \sqrt{|D_{{\bf \mu}}{\bf x}^T({\bf y}, {\bf \mu})) 
D_{{\bf \mu}}{\bf x}({\bf y}, {\bf \mu})) | } \, d{\bf \mu}
}
$$

In the general case, we may have a situation like $y = x^2$. 
Here, there are two manifolds that contribute to the probability distribution of $y$.
More generally, there may be some number of manifolds which map 
onto the target variable. In this case we add the contributions of each 
to the probability distribution of the target variable, $Y$.


\subsection{Applications of Theorem 1.}
We apply Theorem 1 with various types of singular mappings.
\medskip

\example{Example 1}{$y = f({\bf x}) = x_1 + x_2 \quad (f:{\bf R}^2 \mapsto R)$}

The set, $f^{-1}(y)$ can be
parameterized as ${\bf x}_1(\mu; y) = (\mu, y - \mu)^T$ with $\mu \in
(-\infty, \infty)$. 

$ \sqrt{|D_{\mu}{\bf x}_1^T(\mu; y)
\; D_{\mu}{\bf x}_1(\mu; y)|}
= \|(1,-1)\| = \sqrt{2}$. 

Also, $\sqrt{|Df({\bf x}_1(\mu; y)) \;
Df^T({\bf x}_1(\mu; y))|} = \| \nabla_{\bf x}
f({\bf x}_1(\mu; y)) \| = \|(1,1)\| = \sqrt{2}$. 

Therefore, by Theorem 1 we have:

$$
P_Y(y) = \int_{-\infty}^{\infty} P_{X_1 X_2}(\mu, y - \mu) \, d\mu
$$

\example{Example 2}{$y = f({\bf x}) = x_1 / x_2 \quad (f:{\bf R}^2 \mapsto R)$}
The set $f^{-1}(y)$ can be
parameterized as ${\bf x}_1(\mu; y) = (y\mu, \mu)^T$ with $\mu \in
(-\infty, 0) \cup (0, \infty)$. 
We label the first and second components of 
${\bf x}_1$ as $x_{1,1}$ and $x_{1,2}$ respectively.

$ \sqrt{|D_{\mu}{\bf x}_1^T(\mu; y)
\; D_{\mu}{\bf x}_1(\mu; y)|} =
\|(y,1)\| = \sqrt{1 + y^2}$. And,

$$
\eqalign{
\sqrt{|Df({\bf x}_1(\mu; y)) \; Df^T({\bf x}_1(\mu; y))|} & = \|\nabla_{\bf x}  f({\bf x}_1(\mu; y)) \| \cr
& = (1/x_{1,2}(\mu; y), -x_{1,1}(\mu; y)/x_{1,2}(\mu; y)^2)\| \cr
& = \sqrt{1 + y^2} / |\mu|
}
$$

Therefore, Theorem 1 gives:
$$
  P_Y(y) = \int_{-\infty}^{\infty} |\mu| \, P_{X_1 X_2}(y\mu, \mu) \, d\mu
$$

\example{Example 3}{ $y = f({\bf x}) = x_1^2 + x_2^2 \quad 
(f:{\bf R}^2 \mapsto R)$}

The set $f^{-1}(y)$ is the union of the 
parameterizations: ${\bf x}_1(\mu; y) = (\mu, \sqrt{y - \mu^2})^T$ 

and 
${\bf x}_2(\mu; y) = (\mu, - \sqrt{y - \mu^2})^T$ with $\mu \in
[-\sqrt{y}, \sqrt{y}]$. 
 
$ \sqrt{|D_{\mu}{\bf x}_1^T(\mu; y)
\; D_{\mu}{\bf x}_1(\mu; y)|} = \|(1, -\mu/\sqrt{y - \mu^2} \| =
 \sqrt{y/(y - \mu^2)}$. 
 
$ \sqrt{|D_{\mu}{\bf x}_2^T(\mu; y)
\; D_{\mu}{\bf x}_2(\mu; y)|} = \|(1, \mu/\sqrt{y - \mu^2} \| =
 \sqrt{y/(y - \mu^2)}$. 

$\sqrt{|Df({\bf x}_1(\mu; y)) \;
Df^T({\bf x}_1(\mu; y))|} =  2 \| (\mu, \sqrt{y - \mu^2}) \|
=  2 \sqrt{y}$.
 
$\sqrt{|Df({\bf x}_2(\mu; y)) \;
Df^T({\bf x}_2(\mu; y))|} = 2 \|(\mu, -\sqrt{y - \mu^2}) \|
= 2 \sqrt{y}$. 

Therefore, Theorem 1 gives:
$$
  P_Y(y) = \cases{ \int_{-\sqrt{y}}^{\sqrt{y}}  \frac{P_{X_1 X_2}(\mu,
  \sqrt{y - \mu^2}) \; 
+ P_{X_1 X_2}(\mu, -\sqrt{y - \mu^2})}{2 \sqrt{y - \mu^2}} \, d\mu &
  $y\ge0$ ; \cr
0 & otherwise.}
$$


\example{Example 4}{$y_1 = x_1 + x_2 + x_3 \;;\; y_2 = x_1 - x_2 \quad
  (f:{\bf R}^3 \mapsto {\bf R}^2)$}
The set $f^{-1}({\bf y})$ can be
parameterized as ${\bf x}_1(\mu; y_1, y_2) = (\mu, \mu - y_2, y_1 +
y_2 - 2\mu)^T$
with $\mu \in
(-\infty, \infty)$. 

$ \sqrt{|D_{\mu}{\bf x}_1^T(\mu; {\bf y})
\; D_{\mu}{\bf x}_1(\mu; {\bf y})|} =
\|(1, 1, -2)\| = \sqrt{6}$

And, $\sqrt{|Df({\bf x}_1(\mu; {\bf y})) \;
Df^T({\bf x}_1(\mu; {\bf y}))|} = 
\sqrt{\left|\pmatrix{1 & 1 & 1 \cr 1 & -1 & 0\cr} 
\pmatrix{1 & 1 \cr 1 & -1 \cr 1 & 0 \cr} \right|} = \sqrt{6} $

Therefore, Theorem 1 gives:
$$
  P_{Y_1 Y_2}(y_1, y_2) = \int_{-\infty}^{\infty} P_{X_1
  X_2 X_3}(\mu, \mu - y_2, y_1 + y_2 - 2 \mu) \, d\mu
$$
\bigskip


\example{Example 5a}{$y_1 = x_1 + x_2 + x_3 \;;\; y_2 = x_1 - x_2 + x_4 \quad
  (f:{\bf R}^4 \mapsto {\bf R}^2)$}
The set $f^{-1}({\bf y})$ can be
parameterized as 
${\bf x}_1(\mu_1, \mu_2; y_1, y_2) = (\mu_1, \mu_2, y_1 - \mu_1 - \mu_2, y_2 - \mu_1 + \mu_2)^T$ 
with $\mu_1, \mu_2 \in
(-\infty, \infty) $. 

$ \sqrt{|D_{\mu}{\bf x}_1^T(\mu; {\bf y})
\; D_{\mu}{\bf x}_1(\mu; {\bf y})|} =
\sqrt{\left| \pmatrix{1 & 0 & -1 & -1 \cr 
                      0 & 1 & -1 &  1 \cr } 
\pmatrix{1 &  0 \cr 
         0 &  1 \cr 
        -1 & -1 \cr 
        -1 &  1 \cr} \right|} = 3$

And, $\sqrt{|Df({\bf x}_1(\mu; {\bf y})) \;
Df^T({\bf x}_1(\mu; {\bf y}))|} = \sqrt{\left|\pmatrix{1 & 1 &
1 & 0 \cr 1 & -1 & 0 & 1\cr} \pmatrix{1 & 1 \cr 1 & -1 \cr 1 & 0 \cr 0
& 1 \cr} \right|} = 3 $

Therefore, Theorem 1 gives:
$$
  P_{Y_1 Y_2}(y_1, y_2) = \int_{-\infty}^{\infty} 
  \int_{-\infty}^{\infty} P_{X_1 X_2 X_3 X_4}
  (\mu_1, \mu_2, y_1 - \mu_1 -\mu_2, y_2 - \mu_1 + \mu_2)
  \, d\mu_1 \, d\mu_2
$$

\example{Example 5b}{$y_1 = x_1 + x_2 + x_3 \;;\; y_2 = x_1 - x_2 + x_4 \quad
  (f:{\bf R}^4 \mapsto {\bf R}^2)$}
This is the same function as in Example 5a. The difference in this example
is how we parameterize the set $f^{-1}({\bf y})$.
In this example we parameterize this set in the following way:
${\bf x}_1(\mu_1, \mu_2; y_1, y_2) = (2\mu_1, \mu_2, y_1 - 2\mu_1 - \mu_2, y_2 - 2\mu_1 + \mu_2)^T$ 
with $\mu_1, \mu_2 \in
(-\infty, \infty) $. 

Continuing as in 5a we have:
$ \sqrt{|D_{\mu}{\bf x}_1^T(\mu; {\bf y})
\; D_{\mu}{\bf x}_1(\mu; {\bf y})|} =
\sqrt{\left| \pmatrix{2 & 0 & -2 & -2 \cr 
                      0 & 1 & -1 &  1 \cr } 
\pmatrix{2 &  0 \cr 
         0 &  1 \cr 
        -2 & -1 \cr 
        -2 &  1 \cr} \right|} = 6$

And, $\sqrt{|Df({\bf x}_1(\mu; {\bf y})) \;
Df^T({\bf x}_1(\mu; {\bf y}))|} = \sqrt{\left|\pmatrix{1 & 1 &
1 & 0 \cr 1 & -1 & 0 & 1\cr} \pmatrix{1 & 1 \cr 1 & -1 \cr 1 & 0 \cr 0
& 1 \cr} \right|} = 3 $j

Therefore, Theorem 1 gives:
$$
  P_{Y_1 Y_2}(y_1, y_2) = 2\int_{-\infty}^{\infty} 
  \int_{-\infty}^{\infty} P_{X_1 X_2 X_3 X_4}
  (2\mu_1, \mu_2, y_1 - 2\mu_1 -\mu_2, y_2 - 2\mu_1 + \mu_2)
  \, d\mu_1 \, d\mu_2
$$

\example{Example 6}{$y = \sum_{i=1}^n x_i^2 \quad
  (f:{\bf R}^n \mapsto {\bf R})$}
We assume that the $x_i$ are independent normals with mean $0$ and variance $\sigma^2$.
When $\sigma = 1$, $y$ has a Chi-squared distribution of order $n$.
We derive a generalization of this fact using our formula.
The set $f^{-1}(y)$ is a sphere in ${\bf R}^n$ of radius $\sqrt{y}$.
The density function of $y$ is:%
\footnote{\kern -2pt \raise 0.5ex \hbox{\dag}}{%
We use the unparameterized form of the transformation formula, equation (2).
Using the fact that $\Omega_{X_2}$ is $f^{-1}(y)$.}
$$
\eqalignno{
	P_Y(y) & = \int_{\partial B(\sqrt{y})} \frac{P_{\bf X}({\bf x}) \, d{\bf x}}{\sqrt{|Df({\bf x}) \, Df^T({\bf x})|}} & \cr
}
$$
Here, $f({\bf x}) = 2\, {\bf x}$. Consequently, on the sphere of radius, $\sqrt{y}$, we have 
$\sqrt{|Df({\bf x}) Df^T({\bf x})|} = 2 \sqrt{y}$. Therefore, we may write the above as
$$
\eqalignno{
P_Y(y) & = \int_{\partial B(\sqrt{y})} \frac{P_{\bf X}({\bf x}) \, d{\bf x}}{2\sqrt{y}} & \cr
}
$$
This is
$$
\eqalignno{
	P_Y(y) & = \int_{\partial B(\sqrt{y})} \frac{e^{-\sum_{i=1}^n x_i^2/(2\sigma^2)} \, d{\bf x}}{(2\pi)^{n/2} \sigma^n 2\sqrt{y}} & \cr
}
$$
Which is
$$
\eqalignno{
	P_Y(y) & = \int_{\partial B(\sqrt{y})} \frac{e^{-y/(2\sigma^2)} \, d{\bf x}}{(2\pi)^{n/2} \sigma^n 2\sqrt{y}} & \cr
}
$$

Or,
$$
\eqalignno{
	P_Y(y) & = \frac{e^{-y/(2\sigma^2)} \, d{\bf x}}{(2\pi)^{n/2} \sigma^n 2\sqrt{y}} \int_{\partial B(\sqrt{y})} d\, {\bf x}& \cr
}
$$
The surface area of the unit sphere in $n$ dimensions is $2y^{(n-1)/2}\frac{\pi^{n/2}}{\Gamma(n/2)}$.
Therefore, $P_Y(y)$, may be written:
$$
\eqalignno{
	P_Y(y) & = \frac{y^{n/2 - 1}e^{-y/(2\sigma^2)}}{2^{n/2}\sigma^n \Gamma(n/2)} \quad \hbox{$y > 0$.} & \cr
}
$$


\subsection{Application to the Distribution of the Ratio of Normals}
We use Theorem 1 to compute the distribution 
of the ratio of two correlated normals.

If $x_1, x_2$ are Gaussian random variables with joint density function,  
$P_{X_1 X_2}$, means: $\mu_1, \mu_2$, and positive definite covariance 
matrix:\ft 1 {If 
$\Sigma$ is positive definite we have: $a>0, \; c>0, \; ac
- b^2 > 0$.} $\Sigma~=~\pmatrix{a & b \cr b & c \cr}$, then
the density function of the ratio $y = x_1 / x_2$ is given by
the formula in example 2:
$$
P_Y(y) = \int_{-\infty}^{\infty} |s|P_{X_1 X_2}(sy,s) \, ds = 
\frac{1}{2 \pi \sqrt{ac - b^2}} \int_{-\infty}^{\infty} 
|s|e^{-\left([sy - \mu_1, s - \mu_2]
\Sigma^{-1} [sy - \mu_1, s - \mu_2]^T \right)/2} \, ds
$$
This may be reduced to:
$$
\eqalign{
P_Y(y) & =\frac{\sqrt{ac - b^2}e^{-\gamma}}{\pi (cy^2 - 2by + a)} \left[ 1 +
\sqrt{\pi} \tau(y) e^{\tau(y)^2} \, {\rm erf}\left(\tau(y) \right) \right]}
$$

where 
$$
\eqalign{
\gamma & = \frac{c\mu_1^2 - 2b\mu_1\mu_2 + a\mu_2^2}{2(ac - b^2)} \cr
\tau(y) & =
\frac{|y(c\mu_1 - b\mu_2) + a\mu_2 - b\mu_1|}
{\sqrt{cy^2 - 2by + a} \sqrt{2(ac - b^2)}}}
$$

The function $\tau(y)$ is bounded on the interval 
$(-\infty, \infty)$. This follows since by the positive definiteness of 
$\Sigma$, the denominator is bound away from $0$; while, 
for large values of $y$, $\tau(y)$ is $O(1)$.

{\bf Note:} As a consequence, this distribution does not 
have {\it any} moments. This follows since the function $P_Y(y)$ is 
$O(1 / y^2)$, when $|y|$ is large. {\it Therefore, one cannot talk about the 
mean or standard deviation of this distribution.}

This distribution is also not unimodal in general. 

\subsection{Special Cases}

The density function simplifies under the condition: 
$\mu_1 = \mu_2 = 0$. $P_Y(y)$ is given by:
$$
P_Y(y) = \frac{\sqrt{ac - b^2}}{\pi (cy^2 - 2by + a)} 
= \frac{\sqrt{ac - b^2}}{\pi \left( (\sqrt{c}y -
\frac{b}{\sqrt{c}})^2 + (a - \frac{b^2}{c})\right)} 
$$
If, in addition, $x_1$ and $x_2$ are uncorrelated, then $P_Y(y)$
becomes:
$$
P_Y(y) = \frac{\sqrt{ac}}{\pi (cy^2 + a)}
$$
Finally, if we further set, $a = c = 1, b = 0$; that is, $x_1$ and $x_2$
are independent
Gaussians each with mean 0 and variance 1, then $P_Y(y)$ becomes:
$$
P_Y(y) = \frac{1}{\pi (y^2 + 1)}
$$

\bye


